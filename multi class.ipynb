{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.transforms import ToPILImage\n",
    "import torchvision.transforms.functional as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from math import sqrt\n",
    "from PIL import Image\n",
    "from torch.utils import data\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(SEED=42):\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = True # keep True if all the input have same size.\n",
    "SEED=42\n",
    "seed_everything(SEED=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(img):\n",
    "    s = img.size\n",
    "    w_increase = abs((s[0]-max(s))//2)\n",
    "    h_increase = abs((s[1]-max(s))//2)\n",
    "    up_increase = max(s)-s[0]-w_increase\n",
    "    down_increase = max(s)-s[1]-h_increase\n",
    "    padding = (w_increase,h_increase,up_increase,down_increase)\n",
    "    return tf.pad(img, padding)\n",
    "\n",
    "def mytransform1(image,lable,flip=True,jitter=True):\n",
    "    image = pad(image)\n",
    "    mask = pad(lable)\n",
    "    image = tf.resize(image, (224,224))\n",
    "    lable = tf.resize(lable, (224,224)) \n",
    "    if flip == True:\n",
    "        \n",
    "        if random.random() > 0.5:\n",
    "            image = tf.hflip(image)\n",
    "            lable = tf.hflip(lable)\n",
    "        if random.random() > 0.5:\n",
    "            image = tf.vflip(image)\n",
    "            lable = tf.vflip(lable)\n",
    "    if jitter == True:\n",
    "        if  random.random() > 0.7:\n",
    "            \n",
    "            image = tf.adjust_contrast(image,random.uniform(0.9,1.1))\n",
    "            image = tf.adjust_brightness(image,random.uniform(0.7,1.3))\n",
    "    image = tf.to_tensor(image)\n",
    "    lable = np.array(mask)\n",
    "    lable = torch.as_tensor(lable, dtype=torch.int64)\n",
    "\n",
    "    image = tf.normalize(image, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n",
    "    return image, lable\n",
    "\n",
    "def mytransform2(image,lable):\n",
    "    return mytransform1(image,lable,flip=False,jitter=False)\n",
    "data_transform = {'train': mytransform1, 'val': mytransform2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "customize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LipDataset(data.Dataset):\n",
    "    def __init__(self, file_path=None, name=None, transform=None,number=300):   \n",
    "        self.file_path = file_path\n",
    "        self.transform = transform\n",
    "        self.name = name\n",
    "        self.number = number\n",
    "        # initialize\n",
    "        self.init_all_data(file_path) \n",
    "        return None\n",
    "    def init_all_data(self, file_path):\n",
    "        \n",
    "        # record the path\n",
    "        self.images = []\n",
    "        self.lables = []\n",
    "        #because the whole set is too big, I only number to do a trial first\n",
    "        totallist = [line.rstrip('\\n') for line in\n",
    "                        open(os.path.join(self.file_path, 'TrainVal_pose_annotations', 'lip_'+self.name+'_set.csv'), 'r')]\n",
    "        for idx in range(self.number):\n",
    "            tokens = totallist[idx].split(',')\n",
    "            i = tokens[0].split('.')[0]+'.png'\n",
    "            image_path = os.path.join(self.file_path, 'TrainVal_images', 'TrainVal_images', self.name+'_images', tokens[0])\n",
    "            lable_path = os.path.join(self.file_path, 'TrainVal_parsing_annotations', 'TrainVal_parsing_annotations','TrainVal_parsing_annotations',self.name+'_segmentations', i)\n",
    "            if self.is_valid_image(image_path) and self.is_valid_image(lable_path):\n",
    "                self.images.append(image_path)\n",
    "                self.lables.append(lable_path) \n",
    "        return None               \n",
    "    def is_valid_image(self, img_path):\n",
    "        try:\n",
    "            # 若读取成功，设valid为True\n",
    "            i = Image.open(img_path)\n",
    "            valid = True\n",
    "        except:\n",
    "            # 若读取失败，设valid为False\n",
    "            valid = False\n",
    "            \n",
    "        return valid        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "       #turn image to rgb\n",
    "        image = Image.open(self.images[idx]).convert('RGB')\n",
    "        lable = Image.open(self.lables[idx])\n",
    "        if self.transform:\n",
    "            image,lable = self.transform(image,lable)\n",
    "\n",
    "                         \n",
    "        return image, lable   \n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "trainsize = 10000\n",
    "valsize = 300\n",
    "data_name = {'filepath':'../input/singleperson',\n",
    "            'train': ('train',trainsize),\n",
    "             'val': ('val',valsize),\n",
    "               }\n",
    "\n",
    "dataset = {x: LipDataset(data_name['filepath'], data_name[x][0], data_transform[x],data_name[x][1] ) for x in ['train', 'val']}\n",
    "datasize = {x: len(dataset[x]) for x in ['train', 'val']}\n",
    "dataloader = {}\n",
    "val_loader = DataLoader(dataset['val'], batch_size=16, shuffle=True, num_workers=2) \n",
    "train_loader = DataLoader(dataset['train'], batch_size=16, shuffle=True, num_workers=2)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unet and Unet（mobile net version）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "from math import sqrt\n",
    "class Double_Conv_Block(nn.Module):\n",
    "    def __init__(self, input_channel, output_channel):\n",
    "        super(Double_Conv_Block, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_channel, output_channel, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            #if there is padding=1,then no crop\n",
    "            nn.BatchNorm2d(output_channel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(output_channel, output_channel, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(output_channel),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_channel): \n",
    "        return self.conv(input_channel)\n",
    "\n",
    "class Up_Conv_Block(nn.Module): #Up sampling\n",
    "    def __init__(self, input_channel, output_channel):\n",
    "        super(Up_Conv_Block, self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(input_channel, output_channel, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(output_channel),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_channel):\n",
    "        \n",
    "        return self.up(input_channel)\n",
    "\n",
    "\n",
    "class U_Net(nn.Module):\n",
    "    def __init__(self, img_ch=3, output_ch=20):\n",
    "        #img_ch=3 when RGB\n",
    "        #output_ch=1 for our project\n",
    "        super(U_Net, self).__init__()\n",
    "\n",
    "        self.Maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.Conv1 = Double_Conv_Block(input_channel=img_ch, output_channel=64)\n",
    "        self.Conv2 = Double_Conv_Block(input_channel=64, output_channel=128)\n",
    "        self.Conv3 = Double_Conv_Block(input_channel=128, output_channel=256)\n",
    "        self.Conv4 = Double_Conv_Block(input_channel=256, output_channel=512)\n",
    "        self.Conv5 = Double_Conv_Block(input_channel=512, output_channel=1024)\n",
    "\n",
    "        self.Up5 = Up_Conv_Block(input_channel=1024, output_channel=512)\n",
    "        self.Up_Conv_Block5 = Double_Conv_Block(input_channel=1024, output_channel=512)\n",
    "\n",
    "        self.Up4 = Up_Conv_Block(input_channel=512, output_channel=256)\n",
    "        self.Up_Conv_Block4 = Double_Conv_Block(input_channel=512, output_channel=256)\n",
    "\n",
    "        self.Up3 = Up_Conv_Block(input_channel=256, output_channel=128)\n",
    "        self.Up_Conv_Block3 = Double_Conv_Block(input_channel=256, output_channel=128)\n",
    "\n",
    "        self.Up2 = Up_Conv_Block(input_channel=128, output_channel=64)\n",
    "        self.Up_Conv_Block2 = Double_Conv_Block(input_channel=128, output_channel=64)\n",
    "\n",
    "        self.Conv_1x1 = nn.Conv2d(64, output_ch, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "        #initialize weight\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.input_channel\n",
    "                m.weight.data.normal_(0, sqrt(2. / n))\n",
    "                nn.init.constant_(m.bias,0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight,1)\n",
    "                nn.init.constant_(m.bias,0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # downsample \n",
    "        x1 = self.Conv1(x)\n",
    "\n",
    "        x2 = self.Maxpool(x1)\n",
    "        x2 = self.Conv2(x2)\n",
    "\n",
    "        x3 = self.Maxpool(x2)\n",
    "        x3 = self.Conv3(x3)\n",
    "\n",
    "        x4 = self.Maxpool(x3)\n",
    "        x4 = self.Conv4(x4)\n",
    "\n",
    "        x5 = self.Maxpool(x4)\n",
    "        x5 = self.Conv5(x5)\n",
    "\n",
    "        # up sample + concat \n",
    "        d5 = self.Up5(x5)\n",
    "        d5 = torch.cat((x4, d5), dim=1)\n",
    "\n",
    "        d5 = self.Up_Conv_Block5(d5)\n",
    "\n",
    "        d4 = self.Up4(d5)\n",
    "        d4 = torch.cat((x3, d4), dim=1)\n",
    "        d4 = self.Up_Conv_Block4(d4)\n",
    "\n",
    "        d3 = self.Up3(d4)\n",
    "        d3 = torch.cat((x2, d3), dim=1)\n",
    "        d3 = self.Up_Conv_Block3(d3)\n",
    "\n",
    "        d2 = self.Up2(d3)\n",
    "        d2 = torch.cat((x1, d2), dim=1)\n",
    "        d2 = self.Up_Conv_Block2(d2)\n",
    "\n",
    "        d1 = self.Conv_1x1(d2)\n",
    "\n",
    "        return d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "###the mobile net version of Unet\n",
    "from math import sqrt\n",
    "class Double_Conv_Block(nn.Module):\n",
    "    def __init__(self, input_channel, output_channel):\n",
    "        super(Double_Conv_Block, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_channel, output_channel, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            #if there is padding=1,then no crop\n",
    "            nn.BatchNorm2d(output_channel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(output_channel, output_channel, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(output_channel),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_channel): \n",
    "        return self.conv(input_channel)\n",
    "\n",
    "class Up_Conv_Block(nn.Module): #Up sampling\n",
    "    def __init__(self, input_channel, output_channel):\n",
    "        super(Up_Conv_Block, self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(input_channel, output_channel, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(output_channel),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_channel):\n",
    "        \n",
    "        return self.up(input_channel)\n",
    "\n",
    "\n",
    "class U_Net_mobile(nn.Module):\n",
    "    def __init__(self, img_ch=3, output_ch=20):\n",
    "        #img_ch=3 when RGB\n",
    "        #output_ch=1 for our project\n",
    "        super(U_Net_mobile, self).__init__()\n",
    "\n",
    "        self.Maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.Conv1 = Double_Conv_Block(input_channel=img_ch, output_channel=64)\n",
    "        self.Conv2 = Double_Conv_Block(input_channel=64, output_channel=128)\n",
    "        self.Conv3 = Double_Conv_Block(input_channel=128, output_channel=256)\n",
    "        self.Conv4 = Double_Conv_Block(input_channel=256, output_channel=512)\n",
    "        self.Conv5 = Double_Conv_Block(input_channel=512, output_channel=1024)\n",
    "\n",
    "        self.Up5 = Up_Conv_Block(input_channel=1024, output_channel=512)\n",
    "        self.Up_Conv_Block5 = Double_Conv_Block(input_channel=1024, output_channel=512)\n",
    "\n",
    "        self.Up4 = Up_Conv_Block(input_channel=512, output_channel=256)\n",
    "        self.Up_Conv_Block4 = Double_Conv_Block(input_channel=512, output_channel=256)\n",
    "\n",
    "        self.Up3 = Up_Conv_Block(input_channel=256, output_channel=128)\n",
    "        self.Up_Conv_Block3 = Double_Conv_Block(input_channel=256, output_channel=128)\n",
    "\n",
    "        self.Up2 = Up_Conv_Block(input_channel=128, output_channel=64)\n",
    "        self.Up_Conv_Block2 = Double_Conv_Block(input_channel=128, output_channel=64)\n",
    "\n",
    "        self.Conv_1x1 = nn.Conv2d(64, output_ch, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "       #initialize weight\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.input_channel\n",
    "                m.weight.data.normal_(0, sqrt(2. / n))\n",
    "                nn.init.constant_(m.bias,0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight,1)\n",
    "                nn.init.constant_(m.bias,0)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # downsample \n",
    "        x1 = self.Conv1(x)\n",
    "\n",
    "        x2 = self.Maxpool(x1)\n",
    "        x2 = self.Conv2(x2)\n",
    "\n",
    "        x3 = self.Maxpool(x2)\n",
    "        x3 = self.Conv3(x3)\n",
    "\n",
    "        x4 = self.Maxpool(x3)\n",
    "        x4 = self.Conv4(x4)\n",
    "\n",
    "        x5 = self.Maxpool(x4)\n",
    "        x5 = self.Conv5(x5)\n",
    "\n",
    "        # up sample + concat \n",
    "        d5 = self.Up5(x5)\n",
    "        d5 = torch.cat((x4, d5), dim=1)\n",
    "\n",
    "        d5 = self.Up_Conv_Block5(d5)\n",
    "\n",
    "        d4 = self.Up4(d5)\n",
    "        d4 = torch.cat((x3, d4), dim=1)\n",
    "        d4 = self.Up_Conv_Block4(d4)\n",
    "\n",
    "        d3 = self.Up3(d4)\n",
    "        d3 = torch.cat((x2, d3), dim=1)\n",
    "        d3 = self.Up_Conv_Block3(d3)\n",
    "\n",
    "        d2 = self.Up2(d3)\n",
    "        d2 = torch.cat((x1, d2), dim=1)\n",
    "        d2 = self.Up_Conv_Block2(d2)\n",
    "\n",
    "        d1 = self.Conv_1x1(d2)\n",
    "\n",
    "        return d1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "summary module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def summary(model, input_size, batch_size=-1, device=torch.device('cuda:0'), dtypes=None):\n",
    "    result, params_info = summary_string(\n",
    "        model, input_size, batch_size, device, dtypes)\n",
    "    print(result)\n",
    "\n",
    "    return params_info\n",
    "\n",
    "\n",
    "def summary_string(model, input_size, batch_size=-1, device=torch.device('cuda:0'), dtypes=None):\n",
    "    if dtypes == None:\n",
    "        dtypes = [torch.FloatTensor]*len(input_size)\n",
    "\n",
    "    summary_str = ''\n",
    "\n",
    "    def register_hook(module):\n",
    "        def hook(module, input, output):\n",
    "            class_name = str(module.__class__).split(\".\")[-1].split(\"'\")[0]\n",
    "            module_idx = len(summary)\n",
    "\n",
    "            m_key = \"%s-%i\" % (class_name, module_idx + 1)\n",
    "            summary[m_key] = OrderedDict()\n",
    "            summary[m_key][\"input_shape\"] = list(input[0].size())\n",
    "            summary[m_key][\"input_shape\"][0] = batch_size\n",
    "            if isinstance(output, (list, tuple)):\n",
    "                summary[m_key][\"output_shape\"] = [\n",
    "                    [-1] + list(o.size())[1:] for o in output\n",
    "                ]\n",
    "            else:\n",
    "                summary[m_key][\"output_shape\"] = list(output.size())\n",
    "                summary[m_key][\"output_shape\"][0] = batch_size\n",
    "\n",
    "            params = 0\n",
    "            if hasattr(module, \"weight\") and hasattr(module.weight, \"size\"):\n",
    "                params += torch.prod(torch.LongTensor(list(module.weight.size())))\n",
    "                summary[m_key][\"trainable\"] = module.weight.requires_grad\n",
    "            if hasattr(module, \"bias\") and hasattr(module.bias, \"size\"):\n",
    "                params += torch.prod(torch.LongTensor(list(module.bias.size())))\n",
    "            summary[m_key][\"nb_params\"] = params\n",
    "\n",
    "        if (\n",
    "            not isinstance(module, nn.Sequential)\n",
    "            and not isinstance(module, nn.ModuleList)\n",
    "        ):\n",
    "            hooks.append(module.register_forward_hook(hook))\n",
    "\n",
    "    # multiple inputs to the network\n",
    "    if isinstance(input_size, tuple):\n",
    "        input_size = [input_size]\n",
    "\n",
    "    # batch_size of 2 for batchnorm\n",
    "    x = [torch.rand(2, *in_size).type(dtype).to(device=device)\n",
    "         for in_size, dtype in zip(input_size, dtypes)]\n",
    "\n",
    "    # create properties\n",
    "    summary = OrderedDict()\n",
    "    hooks = []\n",
    "\n",
    "    # register hook\n",
    "    model.apply(register_hook)\n",
    "\n",
    "    # make a forward pass\n",
    "    # print(x.shape)\n",
    "    model(*x)\n",
    "\n",
    "    # remove these hooks\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    summary_str += \"----------------------------------------------------------------\" + \"\\n\"\n",
    "    line_new = \"{:>20}  {:>25} {:>15}\".format(\n",
    "        \"Layer (type)\", \"Output Shape\", \"Param #\")\n",
    "    summary_str += line_new + \"\\n\"\n",
    "    summary_str += \"================================================================\" + \"\\n\"\n",
    "    total_params = 0\n",
    "    total_output = 0\n",
    "    trainable_params = 0\n",
    "    for layer in summary:\n",
    "        # input_shape, output_shape, trainable, nb_params\n",
    "        line_new = \"{:>20}  {:>25} {:>15}\".format(\n",
    "            layer,\n",
    "            str(summary[layer][\"output_shape\"]),\n",
    "            \"{0:,}\".format(summary[layer][\"nb_params\"]),\n",
    "        )\n",
    "        total_params += summary[layer][\"nb_params\"]\n",
    "\n",
    "        total_output += np.prod(summary[layer][\"output_shape\"])\n",
    "        if \"trainable\" in summary[layer]:\n",
    "            if summary[layer][\"trainable\"] == True:\n",
    "                trainable_params += summary[layer][\"nb_params\"]\n",
    "        summary_str += line_new + \"\\n\"\n",
    "\n",
    "    # assume 4 bytes/number (float on cuda).\n",
    "    total_input_size = abs(np.prod(sum(input_size, ()))\n",
    "                           * batch_size * 4. / (1024 ** 2.))\n",
    "    total_output_size = abs(2. * total_output * 4. /\n",
    "                            (1024 ** 2.))  # x2 for gradients\n",
    "    total_params_size = abs(total_params * 4. / (1024 ** 2.))\n",
    "    total_size = total_params_size + total_output_size + total_input_size\n",
    "\n",
    "    summary_str += \"================================================================\" + \"\\n\"\n",
    "    summary_str += \"Total params: {0:,}\".format(total_params) + \"\\n\"\n",
    "    summary_str += \"Trainable params: {0:,}\".format(trainable_params) + \"\\n\"\n",
    "    summary_str += \"Non-trainable params: {0:,}\".format(total_params -\n",
    "                                                        trainable_params) + \"\\n\"\n",
    "    summary_str += \"----------------------------------------------------------------\" + \"\\n\"\n",
    "    summary_str += \"Input size (MB): %0.2f\" % total_input_size + \"\\n\"\n",
    "    summary_str += \"Forward/backward pass size (MB): %0.2f\" % total_output_size + \"\\n\"\n",
    "    summary_str += \"Params size (MB): %0.2f\" % total_params_size + \"\\n\"\n",
    "    summary_str += \"Estimated Total Size (MB): %0.2f\" % total_size + \"\\n\"\n",
    "    summary_str += \"----------------------------------------------------------------\" + \"\\n\"\n",
    "    # return summary\n",
    "    return summary_str, (total_params, trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelcross = U_Net().cuda()\n",
    "#modelcross=U_Net_mobile().cuda()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "summary(modelcross,(3, 224, 224),device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(modelcross.parameters(), lr=0.001)###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iou loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Iou(logits, target,n_class=20,device, smooth=1):\n",
    "\n",
    "    logits = logits.cpu().detach()\n",
    "    logits = torch.softmax(logits,dim=1)\n",
    "    pred = torch.max(logits,dim=1)[1].view(-1)\n",
    "    target = target.cpu().detach().view(-1)      \n",
    "    accs=np.zeros(n_class)\n",
    "    for i in range(n_class):        \n",
    "        p = pred==i\n",
    "        t = target==i\n",
    "        intersection = (p * t).sum().numpy()\n",
    "        union = (p + t).sum().numpy()        \n",
    "        accs[i]=(intersection + smooth)/(union + smooth)\n",
    "    return np.mean(accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH=30\n",
    "train_num = datasize['train']\n",
    "val_num = datasize['val']\n",
    "crosslosses=[]#####\n",
    "val_accs=[]\n",
    "train_accs=[]\n",
    "for epoch in range(EPOCH):\n",
    "    # train\n",
    "    modelcross.train()####\n",
    "    \n",
    "\n",
    "    running_loss = 0.0\n",
    "    train_acc=[]\n",
    "    running_acc = 0.0\n",
    "    for step, (images, labels) in enumerate(train_loader, start=0):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = modelcross(images)####\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()* images.size(0)\n",
    "\n",
    "        \n",
    "        running_acc += Iou(preds,labels,20,device)* images.size(0)\n",
    "        train_accurate = running_acc / train_num\n",
    "        \n",
    "        train_acc.append(train_accurate)\n",
    "        # print train process\n",
    "        rate = (step+1)/len(train_loader)\n",
    "        a = \"*\" * int(rate * 50)\n",
    "        b = \".\" * int((1 - rate) * 50)\n",
    "        print(\"\\rtrain loss: {:^3.0f}%[{}->{}]{:.4f}\".format(int(rate*100), a, b, loss), end=\"\")\n",
    "    print()\n",
    "    train_accs.append(train_acc[-1])\n",
    "\n",
    "\n",
    "    # validate\n",
    "    torch.cuda.empty_cache()\n",
    "    modelcross.eval()########\n",
    "    acc = 0.0  # accumulate accurate number / epoch\n",
    "    with torch.no_grad():\n",
    "        for (val_images, val_labels) in val_loader:\n",
    "            val_images, val_labels = val_images.to(device), val_labels.to(device)\n",
    "            outputs = modelcross(val_images)  # eval model only have last output layer\n",
    "            ###\n",
    "            \n",
    "            acc += Iou(outputs,val_labels,20,device)\n",
    "        val_accurate = acc /val_num\n",
    "        val_accs.append(val_accurate)\n",
    "   \n",
    "        print('[epoch %d] train_loss: %.4f  test_accuracy: %.4f train_accuracy: %.4f   ' %\n",
    "              (epoch + 1, running_loss / train_num val_accurate,train_accs[-1]))\n",
    "        crosslosses.append(running_loss / train_num)####\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.arange(len(crosslosses))\n",
    "plt.plot(x,crosslosses,label='cross entropy')\n",
    "plt.title('Loss ')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.arange(len(train_accs))\n",
    "plt.plot(x,train_accs,label='train')\n",
    "plt.plot(x,val_accs,label='validate')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = [\n",
    "    0,   0,   0,\n",
    "   50,   0,   0,   \n",
    "    80,  50,  0,     \n",
    "    30,   30, 90,   \n",
    "    0,   0, 128,     \n",
    "    25,   100, 15,   \n",
    "    20, 148, 128,     \n",
    "    168, 168, 168,   \n",
    "    90,   20,   20,   \n",
    "    192,   15,   15,   \n",
    "    55, 125,   50,    \n",
    "    175, 180,   80,   \n",
    "    60,   5, 125,    \n",
    "    150,   10, 90,   \n",
    "    32, 55, 123,    \n",
    "    192, 128, 125,   \n",
    "    55,  64,   10,     \n",
    "    128,  78,   0,   \n",
    "    10, 220,   80,     \n",
    "    125, 75,   45,    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualset =  torch.utils.data.Subset(dataset['val'], np.arange(4))\n",
    "visualloader = DataLoader(visualset, batch_size=4, shuffle=False)\n",
    "\n",
    "for i in range(4):    \n",
    "    image, lable = visualloader[i]\n",
    "    plt.subplots(i,3,0)\n",
    "    image_show = np.transpose(image_show.numpy(),(1,2,0))\n",
    "    plt.imshow(image_show)\n",
    "    \n",
    "    plt.subplots(i,3,1)\n",
    "    lable_show = torch.as_tensor(lable, dtype=torch.uint8)\n",
    "    lable_show = ToPILImage()(lable_show)\n",
    "    lable_show.putpalette(colormap)\n",
    "    plt.imshow(lable_show)\n",
    "    \n",
    "    plt.subplots(i,3,2)\n",
    "    image = torch.unsqueeze(image,dim=0).to(device)\n",
    "    lable = lable.to(device)\n",
    "    output = modelcross(image)\n",
    "    pred = torch.max(output, 1)[1].cpu().clone()\n",
    "    pred = torch.as_tensor(pred, dtype=torch.uint8)\n",
    "    pred = ToPILImage()(pred)\n",
    "    pred.putpalette(colormap)\n",
    "    plt.imshow(pred)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
