{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils import data\n",
    "from torchvision import transforms as T\n",
    "import torchvision.transforms.functional as tf\n",
    "from torch.utils.data import DataLoader\n",
    "from math import sqrt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(SEED=42):\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = True # keep True if all the input have same size.\n",
    "SEED=42\n",
    "seed_everything(SEED=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(img):\n",
    "    s = img.size\n",
    "    w_increase = abs((s[0]-max(s))//2)\n",
    "    h_increase = abs((s[1]-max(s))//2)\n",
    "    up_increase = max(s)-s[0]-w_increase\n",
    "    down_increase = max(s)-s[1]-h_increase\n",
    "    padding = (w_increase,h_increase,up_increase,down_increase)\n",
    "    return tf.pad(img, padding, fill=0, padding_mode='constant')\n",
    "\n",
    "def mytransform1(image,lable,flip=True,jitter=True):\n",
    "    image = pad(image)\n",
    "    mask = pad(lable)\n",
    "    image = tf.resize(image, (224,224))\n",
    "    lable = tf.resize(lable, (224,224)) \n",
    "    if flip == True:\n",
    "        \n",
    "        if random.random() > 0.5:\n",
    "            image = tf.hflip(image)\n",
    "            lable = tf.hflip(lable)\n",
    "        if random.random() > 0.5:\n",
    "            image = tf.vflip(image)\n",
    "            lable = tf.vflip(lable)\n",
    "    if jitter == True:\n",
    "        if  random.random() > 0.7:\n",
    "            \n",
    "            image = tf.adjust_contrast(image,random.uniform(0.9,1.1))\n",
    "            image = tf.adjust_brightness(image,random.uniform(0.7,1.3))\n",
    "    image = tf.to_tensor(image)\n",
    "    lable = tf.to_tensor(lable)\n",
    "    image = tf.normalize(image, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n",
    "    return image, lable\n",
    "\n",
    "def mytransform2(image,lable):\n",
    "    return mytransform1(image,lable,flip=False,jitter=False)\n",
    "data_transform = {'train': mytransform1, 'val': mytransform2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "customize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LipDataset(data.Dataset):\n",
    "    def __init__(self, file_path=None, name=None, transform=None,number=300):   \n",
    "        self.file_path = file_path\n",
    "        self.transform = transform\n",
    "        self.name = name\n",
    "        self.number = number\n",
    "        # initialize\n",
    "        self.init_all_data(file_path) \n",
    "        return None\n",
    "    def init_all_data(self, file_path):\n",
    "        \n",
    "        # record the path\n",
    "        self.images = []\n",
    "        self.lables = []\n",
    "        #because the whole set is too big, I only number to do a trial first\n",
    "        totallist = [line.rstrip('\\n') for line in\n",
    "                        open(os.path.join(self.file_path, 'TrainVal_pose_annotations', 'lip_'+self.name+'_set.csv'), 'r')]\n",
    "        for idx in range(self.number):\n",
    "            tokens = totallist[idx].split(',')\n",
    "            i = tokens[0].split('.')[0]+'.png'\n",
    "            image_path = os.path.join(self.file_path, 'TrainVal_images', 'TrainVal_images', self.name+'_images', tokens[0])\n",
    "            lable_path = os.path.join(self.file_path, 'TrainVal_parsing_annotations', 'TrainVal_parsing_annotations','TrainVal_parsing_annotations',self.name+'_segmentations', i)\n",
    "            if self.is_valid_image(image_path) and self.is_valid_image(lable_path):\n",
    "                self.images.append(image_path)\n",
    "                self.lables.append(lable_path) \n",
    "        return None               \n",
    "    def is_valid_image(self, img_path):\n",
    "        try:\n",
    "            # 若读取成功，设valid为True\n",
    "            i = Image.open(img_path)\n",
    "            valid = True\n",
    "        except:\n",
    "            # 若读取失败，设valid为False\n",
    "            valid = False\n",
    "            \n",
    "        return valid        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "       #turn image to rgb\n",
    "        image = Image.open(self.images[idx]).convert('RGB')\n",
    "        lable = Image.open(self.lables[idx])\n",
    "        if self.transform:\n",
    "            image,lable = self.transform(image,lable)\n",
    "\n",
    "                         \n",
    "        return image, lable   \n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "trainsize = 10000\n",
    "valsize = 300\n",
    "data_name = {'filepath':'../input/singleperson',\n",
    "            'train': ('train',trainsize),\n",
    "             'val': ('val',valsize),\n",
    "               }\n",
    "\n",
    "dataset = {x: LipDataset(data_name['filepath'], data_name[x][0], data_transform[x],data_name[x][1] ) for x in ['train', 'val']}\n",
    "datasize = {x: len(dataset[x]) for x in ['train', 'val']}\n",
    "dataloader = {}\n",
    "val_loader = DataLoader(dataset['val'], batch_size=16, shuffle=True, num_workers=2) \n",
    "train_loader = DataLoader(dataset['train'], batch_size=16, shuffle=True, num_workers=2)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "from math import sqrt\n",
    "class Double_Conv_Block(nn.Module):\n",
    "    def __init__(self, input_channel, output_channel):\n",
    "        super(Double_Conv_Block, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_channel, output_channel, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            #if there is padding=1,then no crop\n",
    "            nn.BatchNorm2d(output_channel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(output_channel, output_channel, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(output_channel),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_channel): \n",
    "        return self.conv(input_channel)\n",
    "\n",
    "class Up_Conv_Block(nn.Module): #Up sampling\n",
    "    def __init__(self, input_channel, output_channel):\n",
    "        super(Up_Conv_Block, self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(input_channel, output_channel, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(output_channel),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_channel):\n",
    "        \n",
    "        return self.up(input_channel)\n",
    "\n",
    "\n",
    "class U_Net(nn.Module):\n",
    "    def __init__(self, img_ch=3, output_ch=20):\n",
    "        #img_ch=3 when RGB\n",
    "        #output_ch=1 for our project\n",
    "        super(U_Net, self).__init__()\n",
    "\n",
    "        self.Maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.Conv1 = Double_Conv_Block(input_channel=img_ch, output_channel=64)\n",
    "        self.Conv2 = Double_Conv_Block(input_channel=64, output_channel=128)\n",
    "        self.Conv3 = Double_Conv_Block(input_channel=128, output_channel=256)\n",
    "        self.Conv4 = Double_Conv_Block(input_channel=256, output_channel=512)\n",
    "        self.Conv5 = Double_Conv_Block(input_channel=512, output_channel=1024)\n",
    "\n",
    "        self.Up5 = Up_Conv_Block(input_channel=1024, output_channel=512)\n",
    "        self.Up_Conv_Block5 = Double_Conv_Block(input_channel=1024, output_channel=512)\n",
    "\n",
    "        self.Up4 = Up_Conv_Block(input_channel=512, output_channel=256)\n",
    "        self.Up_Conv_Block4 = Double_Conv_Block(input_channel=512, output_channel=256)\n",
    "\n",
    "        self.Up3 = Up_Conv_Block(input_channel=256, output_channel=128)\n",
    "        self.Up_Conv_Block3 = Double_Conv_Block(input_channel=256, output_channel=128)\n",
    "\n",
    "        self.Up2 = Up_Conv_Block(input_channel=128, output_channel=64)\n",
    "        self.Up_Conv_Block2 = Double_Conv_Block(input_channel=128, output_channel=64)\n",
    "\n",
    "        self.Conv_1x1 = nn.Conv2d(64, output_ch, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "        #initialize weight\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.in_channels\n",
    "                m.weight.data.normal_(0, sqrt(2. / n))\n",
    "                nn.init.constant_(m.bias,0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight,1)\n",
    "                nn.init.constant_(m.bias,0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # downsample \n",
    "        x1 = self.Conv1(x)\n",
    "\n",
    "        x2 = self.Maxpool(x1)\n",
    "        x2 = self.Conv2(x2)\n",
    "\n",
    "        x3 = self.Maxpool(x2)\n",
    "        x3 = self.Conv3(x3)\n",
    "\n",
    "        x4 = self.Maxpool(x3)\n",
    "        x4 = self.Conv4(x4)\n",
    "\n",
    "        x5 = self.Maxpool(x4)\n",
    "        x5 = self.Conv5(x5)\n",
    "\n",
    "        # up sample + concat \n",
    "        d5 = self.Up5(x5)\n",
    "        d5 = torch.cat((x4, d5), dim=1)\n",
    "\n",
    "        d5 = self.Up_Conv_Block5(d5)\n",
    "\n",
    "        d4 = self.Up4(d5)\n",
    "        d4 = torch.cat((x3, d4), dim=1)\n",
    "        d4 = self.Up_Conv_Block4(d4)\n",
    "\n",
    "        d3 = self.Up3(d4)\n",
    "        d3 = torch.cat((x2, d3), dim=1)\n",
    "        d3 = self.Up_Conv_Block3(d3)\n",
    "\n",
    "        d2 = self.Up2(d3)\n",
    "        d2 = torch.cat((x1, d2), dim=1)\n",
    "        d2 = self.Up_Conv_Block2(d2)\n",
    "\n",
    "        d1 = self.Conv_1x1(d2)\n",
    "\n",
    "        return d1\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mobile net version Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "from math import sqrt\n",
    "class Double_Conv_Block(nn.Module):\n",
    "    def __init__(self, input_channel, output_channel):\n",
    "        super(Double_Conv_Block, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_channel, output_channel, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            #if there is padding=1,then no crop\n",
    "            nn.BatchNorm2d(output_channel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(output_channel, output_channel, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(output_channel),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_channel): \n",
    "        return self.conv(input_channel)\n",
    "\n",
    "class Up_Conv_Block(nn.Module): #Up sampling\n",
    "    def __init__(self, input_channel, output_channel):\n",
    "        super(Up_Conv_Block, self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(input_channel, output_channel, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(output_channel),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_channel):\n",
    "        \n",
    "        return self.up(input_channel)\n",
    "\n",
    "\n",
    "class U_Net_mobile(nn.Module):\n",
    "    def __init__(self, img_ch=3, output_ch=20):\n",
    "        #img_ch=3 when RGB\n",
    "        #output_ch=1 for our project\n",
    "        super(U_Net_mobile, self).__init__()\n",
    "\n",
    "        self.Maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.Conv1 = Double_Conv_Block(input_channel=img_ch, output_channel=64)\n",
    "        self.Conv2 = Double_Conv_Block(input_channel=64, output_channel=128)\n",
    "        self.Conv3 = Double_Conv_Block(input_channel=128, output_channel=256)\n",
    "        self.Conv4 = Double_Conv_Block(input_channel=256, output_channel=512)\n",
    "        self.Conv5 = Double_Conv_Block(input_channel=512, output_channel=1024)\n",
    "\n",
    "        self.Up5 = Up_Conv_Block(input_channel=1024, output_channel=512)\n",
    "        self.Up_Conv_Block5 = Double_Conv_Block(input_channel=1024, output_channel=512)\n",
    "\n",
    "        self.Up4 = Up_Conv_Block(input_channel=512, output_channel=256)\n",
    "        self.Up_Conv_Block4 = Double_Conv_Block(input_channel=512, output_channel=256)\n",
    "\n",
    "        self.Up3 = Up_Conv_Block(input_channel=256, output_channel=128)\n",
    "        self.Up_Conv_Block3 = Double_Conv_Block(input_channel=256, output_channel=128)\n",
    "\n",
    "        self.Up2 = Up_Conv_Block(input_channel=128, output_channel=64)\n",
    "        self.Up_Conv_Block2 = Double_Conv_Block(input_channel=128, output_channel=64)\n",
    "\n",
    "        self.Conv_1x1 = nn.Conv2d(64, output_ch, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "        #initialize weight\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # downsample \n",
    "        x1 = self.Conv1(x)\n",
    "\n",
    "        x2 = self.Maxpool(x1)\n",
    "        x2 = self.Conv2(x2)\n",
    "\n",
    "        x3 = self.Maxpool(x2)\n",
    "        x3 = self.Conv3(x3)\n",
    "\n",
    "        x4 = self.Maxpool(x3)\n",
    "        x4 = self.Conv4(x4)\n",
    "\n",
    "        x5 = self.Maxpool(x4)\n",
    "        x5 = self.Conv5(x5)\n",
    "\n",
    "        # up sample + concat \n",
    "        d5 = self.Up5(x5)\n",
    "        d5 = torch.cat((x4, d5), dim=1)\n",
    "\n",
    "        d5 = self.Up_Conv_Block5(d5)\n",
    "\n",
    "        d4 = self.Up4(d5)\n",
    "        d4 = torch.cat((x3, d4), dim=1)\n",
    "        d4 = self.Up_Conv_Block4(d4)\n",
    "\n",
    "        d3 = self.Up3(d4)\n",
    "        d3 = torch.cat((x2, d3), dim=1)\n",
    "        d3 = self.Up_Conv_Block3(d3)\n",
    "\n",
    "        d2 = self.Up2(d3)\n",
    "        d2 = torch.cat((x1, d2), dim=1)\n",
    "        d2 = self.Up_Conv_Block2(d2)\n",
    "\n",
    "        d1 = self.Conv_1x1(d2)\n",
    "\n",
    "        return d1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "summary module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def summary(model, input_size, batch_size=-1, device=torch.device('cuda:0'), dtypes=None):\n",
    "    result, params_info = summary_string(\n",
    "        model, input_size, batch_size, device, dtypes)\n",
    "    print(result)\n",
    "\n",
    "    return params_info\n",
    "\n",
    "\n",
    "def summary_string(model, input_size, batch_size=-1, device=torch.device('cuda:0'), dtypes=None):\n",
    "    if dtypes == None:\n",
    "        dtypes = [torch.FloatTensor]*len(input_size)\n",
    "\n",
    "    summary_str = ''\n",
    "\n",
    "    def register_hook(module):\n",
    "        def hook(module, input, output):\n",
    "            class_name = str(module.__class__).split(\".\")[-1].split(\"'\")[0]\n",
    "            module_idx = len(summary)\n",
    "\n",
    "            m_key = \"%s-%i\" % (class_name, module_idx + 1)\n",
    "            summary[m_key] = OrderedDict()\n",
    "            summary[m_key][\"input_shape\"] = list(input[0].size())\n",
    "            summary[m_key][\"input_shape\"][0] = batch_size\n",
    "            if isinstance(output, (list, tuple)):\n",
    "                summary[m_key][\"output_shape\"] = [\n",
    "                    [-1] + list(o.size())[1:] for o in output\n",
    "                ]\n",
    "            else:\n",
    "                summary[m_key][\"output_shape\"] = list(output.size())\n",
    "                summary[m_key][\"output_shape\"][0] = batch_size\n",
    "\n",
    "            params = 0\n",
    "            if hasattr(module, \"weight\") and hasattr(module.weight, \"size\"):\n",
    "                params += torch.prod(torch.LongTensor(list(module.weight.size())))\n",
    "                summary[m_key][\"trainable\"] = module.weight.requires_grad\n",
    "            if hasattr(module, \"bias\") and hasattr(module.bias, \"size\"):\n",
    "                params += torch.prod(torch.LongTensor(list(module.bias.size())))\n",
    "            summary[m_key][\"nb_params\"] = params\n",
    "\n",
    "        if (\n",
    "            not isinstance(module, nn.Sequential)\n",
    "            and not isinstance(module, nn.ModuleList)\n",
    "        ):\n",
    "            hooks.append(module.register_forward_hook(hook))\n",
    "\n",
    "    # multiple inputs to the network\n",
    "    if isinstance(input_size, tuple):\n",
    "        input_size = [input_size]\n",
    "\n",
    "    # batch_size of 2 for batchnorm\n",
    "    x = [torch.rand(2, *in_size).type(dtype).to(device=device)\n",
    "         for in_size, dtype in zip(input_size, dtypes)]\n",
    "\n",
    "    # create properties\n",
    "    summary = OrderedDict()\n",
    "    hooks = []\n",
    "\n",
    "    # register hook\n",
    "    model.apply(register_hook)\n",
    "\n",
    "    # make a forward pass\n",
    "    # print(x.shape)\n",
    "    model(*x)\n",
    "\n",
    "    # remove these hooks\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    summary_str += \"----------------------------------------------------------------\" + \"\\n\"\n",
    "    line_new = \"{:>20}  {:>25} {:>15}\".format(\n",
    "        \"Layer (type)\", \"Output Shape\", \"Param #\")\n",
    "    summary_str += line_new + \"\\n\"\n",
    "    summary_str += \"================================================================\" + \"\\n\"\n",
    "    total_params = 0\n",
    "    total_output = 0\n",
    "    trainable_params = 0\n",
    "    for layer in summary:\n",
    "        # input_shape, output_shape, trainable, nb_params\n",
    "        line_new = \"{:>20}  {:>25} {:>15}\".format(\n",
    "            layer,\n",
    "            str(summary[layer][\"output_shape\"]),\n",
    "            \"{0:,}\".format(summary[layer][\"nb_params\"]),\n",
    "        )\n",
    "        total_params += summary[layer][\"nb_params\"]\n",
    "\n",
    "        total_output += np.prod(summary[layer][\"output_shape\"])\n",
    "        if \"trainable\" in summary[layer]:\n",
    "            if summary[layer][\"trainable\"] == True:\n",
    "                trainable_params += summary[layer][\"nb_params\"]\n",
    "        summary_str += line_new + \"\\n\"\n",
    "\n",
    "    # assume 4 bytes/number (float on cuda).\n",
    "    total_input_size = abs(np.prod(sum(input_size, ()))\n",
    "                           * batch_size * 4. / (1024 ** 2.))\n",
    "    total_output_size = abs(2. * total_output * 4. /\n",
    "                            (1024 ** 2.))  # x2 for gradients\n",
    "    total_params_size = abs(total_params * 4. / (1024 ** 2.))\n",
    "    total_size = total_params_size + total_output_size + total_input_size\n",
    "\n",
    "    summary_str += \"================================================================\" + \"\\n\"\n",
    "    summary_str += \"Total params: {0:,}\".format(total_params) + \"\\n\"\n",
    "    summary_str += \"Trainable params: {0:,}\".format(trainable_params) + \"\\n\"\n",
    "    summary_str += \"Non-trainable params: {0:,}\".format(total_params -\n",
    "                                                        trainable_params) + \"\\n\"\n",
    "    summary_str += \"----------------------------------------------------------------\" + \"\\n\"\n",
    "    summary_str += \"Input size (MB): %0.2f\" % total_input_size + \"\\n\"\n",
    "    summary_str += \"Forward/backward pass size (MB): %0.2f\" % total_output_size + \"\\n\"\n",
    "    summary_str += \"Params size (MB): %0.2f\" % total_params_size + \"\\n\"\n",
    "    summary_str += \"Estimated Total Size (MB): %0.2f\" % total_size + \"\\n\"\n",
    "    summary_str += \"----------------------------------------------------------------\" + \"\\n\"\n",
    "    # return summary\n",
    "    return summary_str, (total_params, trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelcross=U_Net_mobile().cuda()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "summary(modelcross,(3, 224, 224),device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelcross = U_Net().cuda()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#summary(modelcross,(3, 224, 224),device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = DiceLoss(model)\n",
    "#criterion = nn.BCELoss(model)\n",
    "optimizer = optim.Adam(modelcross.parameters(), lr=0.001)###\n",
    "#scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0= 5, T_mult=1, eta_min=0, last_epoch=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "original iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Iou(input,target,classNum,device):\n",
    "    '''\n",
    "    :param input: [b,h,w]\n",
    "    :param target: [b,h,w]\n",
    "    :param classNum: scalar\n",
    "    :return:\n",
    "    '''\n",
    "    inputTmp = torch.zeros([input.shape[0],classNum,input.shape[1],input.shape[2]]).cuda(device)#创建[b,c,h,w]大小的0矩阵\n",
    "    targetTmp = torch.zeros([target.shape[0],classNum,target.shape[1],target.shape[2]]).cuda(device)#同上\n",
    "    input = input.unsqueeze(1).cuda(device)#将input维度扩充为[b,1,h,w]\n",
    "    target = target.unsqueeze(1).cuda(device)#同上\n",
    "    inputOht = inputTmp.scatter_(index=input,dim=1,value=1).cuda(device)#input作为索引，将0矩阵转换为onehot矩阵\n",
    "    targetOht = targetTmp.scatter_(index=target.long(),dim=1,value=1).cuda(device)#同上\n",
    "    batchMious = []#为该batch中每张图像存储一个miou\n",
    "    mul = inputOht * targetOht#乘法计算后，其中1的个数为intersection\n",
    "    for i in range(input.shape[0]):#遍历图像\n",
    "        ious = []\n",
    "        for j in range(classNum):#遍历类别，包括背景\n",
    "            intersection = torch.sum(mul[i][j])\n",
    "            union = torch.sum(inputOht[i][j]) + torch.sum(targetOht[i][j]) - intersection + 1e-6\n",
    "            iou = intersection / union\n",
    "            \n",
    "            \n",
    "            iou = iou.data.cpu().numpy()\n",
    "\n",
    "            ious.append(iou)\n",
    "        miou = np.mean(ious)#计算该图像的miou\n",
    "        batchMious.append(miou)\n",
    "    return np.mean(batchMious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 100%[**************************************************->]0.0033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f948a5a55f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 962, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 942, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f948a5a55f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 962, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 942, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] train_loss: 0.1173  test_accuracy: 0.0018 train_accuracy: 0.0547   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f948a5a55f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 962, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 942, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f948a5a55f0>\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 962, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 942, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 89 %[********************************************->.....]0.00100"
     ]
    }
   ],
   "source": [
    "EPOCH=31\n",
    "train_num = datasize['train']\n",
    "val_num = datasize['val']\n",
    "crosslosses=[]#####\n",
    "val_accs=[]\n",
    "train_accs=[]\n",
    "for epoch in range(EPOCH):\n",
    "    # train\n",
    "    modelcross.train()####\n",
    "    \n",
    "\n",
    "    running_loss = 0.0\n",
    "    train_acc=[]\n",
    "    running_acc = 0.0\n",
    "    for step, data in enumerate(train_loader, start=0):\n",
    "        images, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        logits = modelcross(images.cuda()).to(torch.float64).cuda(device)####\n",
    "\n",
    "        #o = logits.cuda().data.cpu().numpy()\n",
    "        #preds = torch.tensor((o>0.5).astype(np.float32)).to(device)\n",
    "        preds = torch.max(logits,1)[1].cuda(device)\n",
    "        labels = torch.squeeze(labels).cuda(device)\n",
    "        loss = criterion(logits, torch.squeeze(labels).long().cuda())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        \n",
    "        running_acc += Iou(preds,labels,20,device)\n",
    "        train_accurate = running_acc \n",
    "        \n",
    "        train_acc.append(train_accurate)\n",
    "        # print train process\n",
    "        rate = (step+1)/len(train_loader)\n",
    "        a = \"*\" * int(rate * 50)\n",
    "        b = \".\" * int((1 - rate) * 50)\n",
    "        print(\"\\rtrain loss: {:^3.0f}%[{}->{}]{:.4f}\".format(int(rate*100), a, b, loss), end=\"\")\n",
    "    print()\n",
    "    train_accs.append(train_acc[-1])\n",
    "\n",
    "\n",
    "    # validate\n",
    "    torch.cuda.empty_cache()\n",
    "    modelcross.eval()########\n",
    "    acc = 0.0  # accumulate accurate number / epoch\n",
    "    with torch.no_grad():\n",
    "        for val_data in val_loader:\n",
    "            val_images, val_labels = val_data\n",
    "            outputs = modelcross(val_images.cuda()).to(torch.float64).cuda(device)  # eval model only have last output layer\n",
    "            ###\n",
    "            \n",
    "            \n",
    "            # loss = criterion(outputs, test_labels)\n",
    "            #o = outputs.cuda().data.cpu().numpy()\n",
    "            #preds = torch.tensor((o>0.5).astype(np.float32)).to(device)\n",
    "            preds = torch.max(outputs,1)[1].cuda(device)\n",
    "            val_labels = torch.squeeze(val_labels).cuda(device)\n",
    "            visualdict = {'image': val_images, 'lable': val_labels, 'pred': preds}\n",
    "\n",
    "\n",
    "            acc += Iou(preds,val_labels,20,device)\n",
    "        val_accurate = acc \n",
    "        val_accs.append(val_accurate)\n",
    "   \n",
    "        print('[epoch %d] train_loss: %.4f  test_accuracy: %.4f train_accuracy: %.4f   ' %\n",
    "              (epoch + 1, running_loss / step, val_accurate,train_accs[-1]))\n",
    "        crosslosses.append(running_loss / step)####\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.arange(len(crosslosses))\n",
    "plt.plot(x,crosslosses,label='cross entropy')\n",
    "plt.title('Loss ')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.arange(len(train_accs))\n",
    "plt.plot(x,train_accs,label='train')\n",
    "plt.plot(x,val_accs,label='validate')\n",
    "plt.title('DiceBCE Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changecolor(image):\n",
    "    c=255/19\n",
    "    for i in range(image.size()[1]):\n",
    "        for j in range(image.size()[2]):\n",
    "            if image[0][i][j]!=0:\n",
    "                image[0][i][j] = image[0][i][j]*c\n",
    "    image = image.cpu().numpy().reshape(224,224)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualset =  torch.utils.data.Subset(dataset['val'], np.arange(4))\n",
    "visualloader = DataLoader(visualset, batch_size=4, shuffle=False)\n",
    "lables = []\n",
    "preds = []\n",
    "images = []\n",
    "for data in visualloader:\n",
    "    image, lable = data\n",
    "\n",
    "    output = modelcross(image.cuda()).cuda(device).to(torch.float64)###\n",
    "\n",
    "    output = torch.max(output,1)[1].cuda(device)\n",
    "    \n",
    "    pred = torch.squeeze(lable).cuda(device).reshape(4,1,224,224)\n",
    "\n",
    "    label = torch.squeeze(lable).cuda(device).reshape(4,1,224,224)\n",
    "    visualdict = {'image': image, 'lable': label, 'pred': output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,3,1)\n",
    "plt.title(\"image\")\n",
    "plt.imshow(np.transpose(image[0].numpy(),(1,2,0)))\n",
    "plt.subplot(1,3,2)\n",
    "plt.title(\"lable\")\n",
    "plt.imshow(lable[0].reshape(224,224).numpy())\n",
    "plt.subplot(1,3,3)\n",
    "plt.title(\"pred\")\n",
    "plt.imshow(pred[0].cpu().reshape(224,224).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
